{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBN Structure Inference\n",
    "\n",
    "The idea is to infer a posterior for the *structure* of a Dynamic Bayesian Network (DBN), given some data.\n",
    "\n",
    "We formulate this task with the following model:\n",
    "\n",
    "$$ P(G | X) \\propto P(X | G) \\cdot P(G) $$\n",
    "\n",
    "* $P(G)$ is a prior distribution over DBN structures. We'll assume it has the form\n",
    "$$P(G) \\propto \\exp \\left( -\\lambda |G \\setminus G^\\prime| \\right)$$\n",
    "where $|G \\setminus G^\\prime|$ denotes the number of edges in the graph, which are not present in some reference graph $G^\\prime$.\n",
    "* $P(X | G)$ is the marginal likelihood of the DBN structure. That is, it's the likelihood of the DBN after the network parameters have been integrated out -- it scores network *structure*. \n",
    "* If we assume some reasonable priors for network parameters, $P(X|G)$ can be obtained in closed form. In this work, we'll use the following marginal likelihood:\n",
    "    \n",
    "    $$P(X | G) \\propto \\prod_{i=1}^p (1 + n)^{-(2^{|\\pi(i)|} - 1)/2} \\left( X_i^{+ T} X_i^+ - \\frac{n}{n+1} X_i^{+ T} B_i (B_i^T B_i)^{-1} B_i^T X_i^+ \\right)^{-\\frac{n}{2}}$$ \n",
    "    where $X$ and $B$ are matrices obtained from data. This marginal likelihood results from an empirical prior over the regression coefficients, and an improper ($\\propto 1/\\sigma^2$) prior for the regression \"noise\" variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /home/dmerrell/.julia/compiled/v1.2/CSV/HHBkp.ji for CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b]\n",
      "└ @ Base loading.jl:1240\n"
     ]
    }
   ],
   "source": [
    "include(\"DiGraph.jl\")\n",
    "using Gen\n",
    "using PyPlot\n",
    "using .DiGraphs\n",
    "using LinearAlgebra\n",
    "using CSV\n",
    "using DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some data\n",
    "\n",
    "For now, we'll work with some data used by Hill et al. in their 2007 paper, _Bayesian Inference of Signaling Network Topology in a Cancer Cell Line_.\n",
    "\n",
    "It gives the differential phosphorylation levels of 20 proteins, in a cancer cell line perturbed by EGF. This is a well-studied signaling pathway; the goal is to produce a graph describing the dependencies between proteins in this pathway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_names = CSV.read(\"data/protein_names.csv\");\n",
    "reference_adjacency = CSV.read(\"data/prior_graph.csv\");\n",
    "timesteps = CSV.read(\"data/time.csv\")\n",
    "timeseries_data = CSV.read(\"data/mukherjee_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "Implement the graph prior distribution:\n",
    "\n",
    "$$P(G) \\propto \\exp \\left( -\\lambda |G \\setminus G^\\prime| \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct GraphPrior <: Gen.Distribution{DiGraph} end\n",
    "const graphprior = GraphPrior()\n",
    "\n",
    "Gen.random(gp::GraphPrior, lambda::Float64, reference_graph::DiGraph) = reference_graph\n",
    "\n",
    "function graph_edge_diff(g::DiGraph, g_ref::DiGraph)\n",
    "    e1 = Set([g.edges[i,:] for i=1:size(g.edges)[1]])\n",
    "    e_ref = Set([g_ref.edges[i,:] for i=1:size(g_ref.edges)[1]])\n",
    "    return length(setdiff(e1, e_ref))\n",
    "end\n",
    "    \n",
    "Gen.logpdf(gp::GraphPrior, graph::DiGraph, lambda::Float64, reference_graph::DiGraph) = -lambda * graph_edge_diff(graph, reference_graph)\n",
    "\n",
    "graphprior(lambda::Float64, reference_graph::DiGraph) = Gen.random(graphprior, lambda, reference_graph);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the DBN's marginal distribution:\n",
    "\n",
    "$$P(X | G) \\propto \\prod_{i=1}^p (1 + n)^{-(2^{|\\pi(i)|} - 1)/2} \\left( X_i^{+ T} X_i^+ - \\frac{n}{n+1} X_i^{+ T} B_i (B_i^T B_i)^{-1} B_i^T X_i^+ \\right)^{-\\frac{n}{2}}$$\n",
    "\n",
    "Some things to note:\n",
    "* We're kind of shoe-horning this marginal likelihood into Gen. The probabilistic programming ethos entails modeling the entire data-generating process. This ought to provide better performance during inference, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct DBNMarginal <: Gen.Distribution{Array{Float64,3}} end\n",
    "const dbnmarginal = DBNMarginal()\n",
    "\n",
    "\"\"\"\n",
    "DBNMarginal's sampling method does nothing.\n",
    "In our inference task, the Xs will always be observed.\n",
    "\"\"\"\n",
    "Gen.random(dbn::DBNMarginal, parents::Vec{Vec{Int}}, T::Int) = zeros(length(parents), T)\n",
    "\n",
    "function compute_B(idx::Tuple{Vararg{Int64,N} where N}, dataset::Array{Array{Float64,2},1},\n",
    "                   B_cache::Dict{Tuple{Vararg{Int64,N} where N}, Array{Float64,2}})\n",
    "\n",
    "function compute_lp_term(X::Array{Float64,2}, dataset::Array{Array{Float64,2},1}, i::Int, T::Int, parents::Array{Int,1},\n",
    "                         lp_cache::Dict{Tuple{Vararg{Int64,N} where N}, Float64},\n",
    "                         B_cache::Dict{Tuple{Vararg{Int64,N} where N}})\n",
    "\n",
    "    # Check whether the term has already been computed:\n",
    "    k = (i, parents...)\n",
    "    if k in keys(lp_cache)\n",
    "        return lp_cache[k]\n",
    "    else\n",
    "        \n",
    "        x_plus = X[i,2:T]\n",
    "        x_minus = X[i,1:T-1]\n",
    "        B = compute_B()\n",
    "        \n",
    "        matprod = pinv(dot(transpose(B), B))\n",
    "        \n",
    "        cache[k] = -0.5*size(B)[2]*log(1.0+N) -  0.5*N*log(dot(transpose(x_plus), x_plus) - (n/(n+1.0))*t\n",
    "        \n",
    "end\n",
    "    \n",
    "\"\"\"\n",
    "DBNMarginal's log_pdf, in effect, returns a score for the \n",
    "network topology. We use a dictionary to cache precomputed terms of the sum.\n",
    "\"\"\"\n",
    "function Gen.log_pdf(dbn::DBNMarginal, X::Array{Float64,3}, parents::Vec{Vec{Int}}, \n",
    "                     cache::Dict{Tuple{Int,Vararg{Int64,N} where N},Float64})\n",
    "    \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a DBN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function dbn_node(x_prev::Vec{Float64}, parents::Vec{Int}, weights::Vec{Float}, noise::Float64)\n",
    "    x_new = @trace(Gen.normal(dot()), :x)\n",
    "    return x_new\n",
    "end\n",
    "\n",
    "layer_nodes = Gen.Map(dbn_node)\n",
    "\n",
    "@gen function dbn_layer(timestep::Int, x_prev::Vec{Float64}, parents::Vec{Vec{Int}}, weights::Vec{Vec{Float64}})\n",
    "    x_prev_repeat = fill(x_prev, length(parents))\n",
    "    x_new = @trace(layer_nodes(x_prev_repeat, parents, weights), :variables)\n",
    "    return x_new\n",
    "end\n",
    "    \n",
    "dbn = Gen.Unfold(dbn_layer)\n",
    "\n",
    "independent_series = Gen.Map(unfolded_layers)\n",
    "\n",
    "@gen function dbn(V::Vec{Int}, parents::Vec{Vec{Int}}, T::Int)\n",
    "    \n",
    "    x = @trace(unfolded_layers(), :timeseries)\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement our overall model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_parent_vecs(G)\n",
    "    return [sort(in_neighbors(G, v)) for v in sort(G.vertices)]\n",
    "end\n",
    "    \n",
    "@gen function coeff_prior()\n",
    "\n",
    "@gen function data_generator(X::Vec{Array{Float64,2}}, reference_graph::DiGraph{Int}, Tvec::Vec{Int})\n",
    "    \n",
    "    lambda = @trace(Gen.gamma(1,1), :lambda)\n",
    "    \n",
    "    G = @trace(GraphPrior(lambda, reference_graph), :G)\n",
    "    V = sort(G.vertices)\n",
    "    parents = get_parent_vecs(G)\n",
    "    \n",
    "    x_init = @trace(Gen.mvnormal(), :x_init)\n",
    "    regression_coeffs = @trace(, :beta)\n",
    "    regression_noise = @trace(, :noise)\n",
    "    \n",
    "    x = @trace(dbn(V, parents, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "### Metropolis-Hastings over directed graphs\n",
    "\n",
    "Proposal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicDSLFunction{Any}(Dict{Symbol,Any}(), Dict{Symbol,Any}(), Type[Any, Float64], ##digraph_proposal#371, Bool[0, 0], false)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Proposal distribution for exploring the unconstrained space of\n",
    "directed graphs.\n",
    "\n",
    "`expected_indegree` guides the exploration -- if a vertex's in-degree\n",
    "is higher than this, then we are much more likely to remove an edge\n",
    "from one of its parents.\n",
    "\"\"\"\n",
    "@gen function digraph_proposal(tr, expected_indegree::Float64)\n",
    "    \n",
    "    G = copy(t[:G])\n",
    "    ordered_vertices = sort(G.vertices)\n",
    "    V = length(G.vertices)\n",
    "    \n",
    "    u_idx = @trace(Gen.categorical(V), :u_idx)\n",
    "    u = ordered_vertices[u_idx]\n",
    "    ordered_inneighbors = sort(in_neighbors(G,u))\n",
    "    in_deg = length(ordered_inneighbors)\n",
    "    \n",
    "    prob_remove = (in_deg / V) ^ log2(1.0*V / expected_degree)\n",
    "    remove_edge = @trace(Gen.bernoulli(prob_remove), :remove_edge)\n",
    "    \n",
    "    if remove_edge\n",
    "        v_idx = @trace(Gen.categorical(in_deg), :v_idx)\n",
    "        v = ordered_inneighbors[v_idx]\n",
    "        remove_edge!(G, v, u)\n",
    "    else\n",
    "        \n",
    "        ordered_outneighbors = sort(out_neighbors(G,u))\n",
    "        out_deg = length(ordered_outneighbors)\n",
    "        neighbors = union(ordered_inneighbors, ordered_outneighbors)\n",
    "        deg = length(neighbors)\n",
    "        \n",
    "        prob_reverse = (out_deg / deg)\n",
    "        reverse_edge = @trace(Gen.bernoulli(prob_remove), :reverse_edge)\n",
    "        if reverse_edge\n",
    "            \n",
    "            v_idx = @trace(Gen.categorical(out_deg), :v_idx)\n",
    "            v = ordered_outneighbors[v_idx]\n",
    "            remove_edge!(G, u, v)\n",
    "            add_edge!(G, v, u)\n",
    "        else\n",
    "            nonparents = sort(setdiff(G.vertices, inneighbors))\n",
    "            \n",
    "            v_idx = @trace(Gen.categorical(length(nonparents)), :v_idx)\n",
    "            v = nonneighbors[v_idx]\n",
    "            add_edge!(G, v, u)\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    \n",
    "    return G\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Involution function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function digraph_involution(cur_tr, fwd_choices, fwd_ret, prop_args)\n",
    "    \n",
    "    # Update the trace \n",
    "    new_G = fwd_ret\n",
    "    old_G = cur_tr[:G]\n",
    "    update_choices = Gen.choicemap()\n",
    "    update_choices[:G] = new_G\n",
    "    new_tr, weight, retdiff, discard = Gen.update(cur_tr, Gen.get_args(cur_tr), (), update_choices)\n",
    "    \n",
    "    # figure out what has changed\n",
    "    fwd_u_idx = fwd_choices[:u_idx]\n",
    "    sorted_vertices = sort(old_G.vertices)\n",
    "    fwd_u = sorted_vertices[fwd_u_idx]\n",
    "    fwd_v_idx = fwd_choices[:v_idx]\n",
    "    \n",
    "    # Deduce the correct backward choices\n",
    "    bwd_choices = Gen.choicemap()\n",
    "    if fwd_choices[:remove_edge] # an edge was removed -- we must add it back.\n",
    "        \n",
    "        fwd_parents = in_neighbors(old_G, fwd_u)\n",
    "        fwd_v = sort(fwd_parents)[fwd_v_idx]\n",
    "        bwd_nonparents = sort(setdiff(new_G.vertices,fwd_parents))\n",
    "        bwd_v_idx = indexin(fwd_v, bwd_nonparents)[1]\n",
    "        \n",
    "        bwd_choices[:u_idx] = fwd_u_idx \n",
    "        bwd_choices[:remove_edge] = false\n",
    "        bwd_choices[:reverse_edge] = false\n",
    "        bwd_choices[:v_idx] = bwd_v_idx\n",
    "        \n",
    "    else\n",
    "        if fwd_choices[:reverse_edge] # an edge was reversed -- reverse it back.\n",
    "            \n",
    "            \n",
    "            \n",
    "            bwd_choices[:u_idx] = indexin()[1]\n",
    "            bwd_choices[:remove_edge] = false\n",
    "            bwd_choices[:reverse_edge] = true\n",
    "            bwd_choices[:v_idx] = \n",
    "            \n",
    "        else # an edge was added -- remove it.\n",
    "            \n",
    "            fwd_parents = sort(in_neighbors(old_G, fwd_u))\n",
    "            fwd_v = fwd_parents[fwd_v_idx]\n",
    "            bwd_v_idx = indexin(fwd_v, fwd_parents)[1]\n",
    "            \n",
    "            bwd_choices[:u_idx] = fwd_u_idx\n",
    "            bwd_choices[:remove_edge] = true\n",
    "            bwd_choices[:v_idx] = bwd_v_idx\n",
    "            \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return new_tr, bwd_choices, weight\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "parents = [1;2;3;4;5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(i, parents...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dict{Tuple{Vararg{Int64,N} where N}, Float64}()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[(4,5)] = 1.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[()] = 48.281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div(length((1,2,3)),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = union([1;2;3],[4;1;8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexin(8,a)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
