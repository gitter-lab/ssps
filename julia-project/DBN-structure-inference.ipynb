{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBN Structure Inference\n",
    "\n",
    "The idea is to infer a posterior for the *structure* of a Dynamic Bayesian Network (DBN), given some data.\n",
    "\n",
    "We formulate this task with the following model:\n",
    "\n",
    "$$ P(G | X) \\propto P(X | G) \\cdot P(G) $$\n",
    "\n",
    "* $P(G)$ is a prior distribution over DBN structures. We'll assume it has the form\n",
    "$$P(G) \\propto \\exp \\left( -\\lambda |G \\setminus G^\\prime| \\right)$$\n",
    "where $|G \\setminus G^\\prime|$ denotes the number of edges in the graph, which are not present in some reference graph $G^\\prime$.\n",
    "* $P(X | G)$ is the marginal likelihood of the DBN structure. That is, it's the likelihood of the DBN after the network parameters have been integrated out -- it scores network *structure*. \n",
    "* If we assume some reasonable priors for network parameters, $P(X|G)$ can be obtained in closed form. In this work, we'll use the following marginal likelihood:\n",
    "    \n",
    "    $$P(X | G) \\propto \\prod_{i=1}^p (1 + n)^{-(2^{|\\pi(i)|} - 1)/2} \\left( X_i^{+ T} X_i^+ - \\frac{n}{n+1} X_i^{+ T} B_i (B_i^T B_i)^{-1} B_i^T X_i^+ \\right)^{-\\frac{n}{2}}$$ \n",
    "    where $X$ and $B$ are matrices obtained from data; and $n$ is the total number of timesteps in the dataset. This marginal likelihood results from an empirical prior over the regression coefficients, and an improper ($\\propto 1/\\sigma^2$) prior for the regression \"noise\" variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module DiGraphs.\n",
      "WARNING: using DiGraphs.add_edge! in module Main conflicts with an existing identifier.\n",
      "WARNING: using DiGraphs.DiGraph in module Main conflicts with an existing identifier.\n",
      "WARNING: using DiGraphs.transpose! in module Main conflicts with an existing identifier.\n",
      "┌ Info: Precompiling Combinatorics [861a8166-3701-5b0c-9a16-15d98fcdc6aa]\n",
      "└ @ Base loading.jl:1242\n"
     ]
    }
   ],
   "source": [
    "include(\"DiGraph.jl\")\n",
    "using Gen\n",
    "using Plots\n",
    "Plots.pyplot()\n",
    "using .DiGraphs\n",
    "using LinearAlgebra\n",
    "using CSV\n",
    "using DataFrames\n",
    "using LRUCache\n",
    "using Combinatorics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some data\n",
    "\n",
    "For now, we'll work with some data used by Hill et al. in their 2007 paper, _Bayesian Inference of Signaling Network Topology in a Cancer Cell Line_.\n",
    "\n",
    "It gives the differential phosphorylation levels of 20 proteins, in a cancer cell line perturbed by EGF. This is a well-studied signaling pathway; the goal is to produce a graph describing the dependencies between proteins in this pathway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_names = CSV.read(\"data/protein_names.csv\");\n",
    "protein_vec = convert(Matrix, protein_names)[:,1];\n",
    "protein_vec = [name[3:length(name)-2] for name in protein_vec]\n",
    "reference_adjacency = CSV.read(\"data/prior_graph.csv\");\n",
    "adj_mat = convert(Matrix, reference_adjacency);\n",
    "timesteps = CSV.read(\"data/time.csv\");\n",
    "timeseries_data = CSV.read(\"data/mukherjee_data.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "function build_reference_graph(vertices::Vector{T}, reference_adj::Array{Int,2}) where T\n",
    "    dg = DiGraph{T}()\n",
    "    for i=1:size(reference_adj)[1]\n",
    "        for j=1:size(reference_adj)[2]\n",
    "            if reference_adj[i,j] == 1\n",
    "                add_edge!(dg, vertices[i], vertices[j])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return dg\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dg = build_reference_graph(protein_vec, adj_mat);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "Implement the graph prior distribution:\n",
    "\n",
    "$$P(G) \\propto \\exp \\left( -\\lambda |G \\setminus G^\\prime| \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct GraphPrior <: Gen.Distribution{DiGraph} end\n",
    "const graphprior = GraphPrior()\n",
    "\n",
    "Gen.random(gp::GraphPrior, lambda::Float64, reference_graph::DiGraph) = reference_graph\n",
    "\n",
    "function graph_edge_diff(g::DiGraph, g_ref::DiGraph)\n",
    "    e1 = Set([g.edges[i,:] for i=1:size(g.edges)[1]])\n",
    "    e_ref = Set([g_ref.edges[i,:] for i=1:size(g_ref.edges)[1]])\n",
    "    return length(setdiff(e1, e_ref))\n",
    "end\n",
    "    \n",
    "Gen.logpdf(gp::GraphPrior, graph::DiGraph, lambda::Float64, reference_graph::DiGraph) = -lambda * graph_edge_diff(graph, reference_graph)\n",
    "\n",
    "graphprior(lambda::Float64, reference_graph::DiGraph) = Gen.random(graphprior, lambda, reference_graph);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the DBN's marginal distribution:\n",
    "\n",
    "$$P(X | G) \\propto \\prod_{i=1}^p (1 + n)^{-(2^{|\\pi(i)|} - 1)/2} \\left( X_i^{+ T} X_i^+ - \\frac{n}{n+1} X_i^{+ T} B_i (B_i^T B_i)^{-1} B_i^T X_i^+ \\right)^{-\\frac{n}{2}}$$\n",
    "\n",
    "Some things to note:\n",
    "* We're kind of shoe-horning this marginal likelihood into Gen. The probabilistic programming ethos entails modeling the entire data-generating process. This ought to provide better performance during inference, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct DBNMarginal <: Gen.Distribution{Array{Float64,3}} end\n",
    "const dbnmarginal = DBNMarginal()\n",
    "\n",
    "\"\"\"\n",
    "DBNMarginal's sampling method does nothing.\n",
    "In our inference task, the Xs will always be observed.\n",
    "\"\"\"\n",
    "Gen.random(::DBNMarginal, parents::Vector{Vector{Int}}, N::Int, Ts::Vector{Int}) = [zeros(length(parents), T) for i=1:N]\n",
    "\n",
    "# We use a few levels of LRU caching to reduce redundant computation.\n",
    "const TENMB = 10000000\n",
    "\n",
    "# Store computed log-marginal likelihoods: log P(x_i | parents(x_i))\n",
    "const lp_cache = LRU{Tuple{Vararg{Int64}},Float64}(maxsize=TENMB, by=Base.summarysize)\n",
    "\n",
    "# Store constructed B columns. There are combinatorially-many possible columns, and hence\n",
    "# may be expensive to compute. And B columns may be constructed from other B columns\n",
    "# that have already been computed (i.e., caching may be useful.)\n",
    "const B_col_cache = LRU{Tuple{Vararg{Int}},Vector{Float64}}(maxsize=10*TENMB, by=Base.summarysize)\n",
    "\n",
    "# Store the value of B inv(B^T B) B^T; may be reused when different children have the same parents. \n",
    "const B2inv_cache = LRU{Tuple{Vararg{Int}},Array{Float64,2}}(maxsize=TENMB, by=Base.summarysize)\n",
    "\n",
    "function construct_B(parent_inds::Vector{Int64}, Xminus::Array{Float64,2})\n",
    "    len = length(parent_inds)\n",
    "    if len == 1\n",
    "        return Xminus[:,parent_inds]\n",
    "    else\n",
    "        half = len/2\n",
    "        Bleft = construct_B(parent_inds[1:half], Xminus)\n",
    "        Bright = construct_B(parent_inds[1-half:len], Xminus)\n",
    "        \n",
    "    return B\n",
    "end\n",
    "\n",
    "function construct_B2inv(parent_inds::Vector{Int64}, Xminus::Array{Float64,2})\n",
    "    B = construct_B(parent_inds, Xminus)\n",
    "    return LinearAlgebra.inv(LinearAlgebra.Symmetric(LinearAlgebra.dot(transpose(B), B)))\n",
    "end\n",
    "\n",
    "function combine_X(X::Vector{Array{Float64,2}})\n",
    "    \n",
    "    return Xminus, Xplus\n",
    "end\n",
    "\n",
    "function log_marg_lik(Xplus::Vector{Array{Float64,2}}, ind::Int, parent_inds::Vector{Int})\n",
    "\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "DBNMarginal's log_pdf, in effect, returns a score for the \n",
    "network topology. We use a dictionary to cache precomputed terms of the sum.\n",
    "\"\"\"\n",
    "function Gen.log_pdf(dbn::DBNMarginal, X::Array{Float64,3}, parents::Vector{Vector{Int}}, \n",
    "                     cache::Dict{Tuple{Int,Vararg{Int64,N} where N},Float64})\n",
    "    \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearAlgebra.Symmetric()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement our overall model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function data_generator(X::Vec{Array{Float64,2}}, reference_graph::DiGraph{Int}, Tvec::Vec{Int})\n",
    "    \n",
    "    lambda = @trace(Gen.gamma(1,1), :lambda)\n",
    "    \n",
    "    G = @trace(GraphPrior(lambda, reference_graph), :G)\n",
    "    V = sort(G.vertices)\n",
    "    parents = get_parent_vecs(G)\n",
    "    \n",
    "    x_init = @trace(Gen.mvnormal(), :x_init)\n",
    "    regression_coeffs = @trace(, :beta)\n",
    "    regression_noise = @trace(, :noise)\n",
    "    \n",
    "    x = @trace(dbn(V, parents, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "### Metropolis-Hastings over directed graphs\n",
    "\n",
    "Proposal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Proposal distribution for exploring the unconstrained space of\n",
    "directed graphs.\n",
    "\n",
    "`expected_indegree` guides the exploration -- if a vertex's in-degree\n",
    "is higher than this, then we are much more likely to remove an edge\n",
    "from one of its parents.\n",
    "\"\"\"\n",
    "@gen function digraph_proposal(tr, expected_indegree::Float64)\n",
    "    \n",
    "    G = copy(tr[:G])\n",
    "    ordered_vertices = sort(collect(G.vertices))\n",
    "    V = length(G.vertices)\n",
    "    \n",
    "    u_idx = @trace(Gen.uniform_discrete(1,V), :u_idx)\n",
    "    u = ordered_vertices[u_idx]\n",
    "    ordered_inneighbors = sort(collect(in_neighbors(G,u)))\n",
    "    in_deg = length(ordered_inneighbors)\n",
    "    \n",
    "    prob_remove = (1.0*in_deg / V) ^ (-1.0/log2(1.0*expected_indegree/V))\n",
    "    remove_edge = @trace(Gen.bernoulli(prob_remove), :remove_edge)\n",
    "    \n",
    "    if remove_edge\n",
    "        v_idx = @trace(Gen.uniform_discrete(1, in_deg), :v_idx)\n",
    "        v = ordered_inneighbors[v_idx]\n",
    "        remove_edge!(G, v, u)\n",
    "    else\n",
    "        \n",
    "        outneighbors = out_neighbors(G,u)\n",
    "        ordered_exc_outneighbors = sort(collect(setdiff(outneighbors, ordered_inneighbors)))\n",
    "        out_exc_deg = length(ordered_exc_outneighbors)\n",
    "        deg = length(union(outneighbors, ordered_inneighbors))\n",
    "        \n",
    "        prob_reverse = 1.0 * out_exc_deg / deg\n",
    "        reverse_edge = @trace(Gen.bernoulli(prob_reverse), :reverse_edge)\n",
    "        if reverse_edge\n",
    "            \n",
    "            v_idx = @trace(Gen.uniform_discrete(1, out_exc_deg), :v_idx)\n",
    "            v = ordered_exc_outneighbors[v_idx]\n",
    "            remove_edge!(G, u, v)\n",
    "            add_edge!(G, v, u)\n",
    "        else\n",
    "            nonparents = sort(collect(setdiff(G.vertices, ordered_inneighbors)))\n",
    "            len = length(nonparents)\n",
    "            v_idx = @trace(Gen.uniform_discrete(1,len), :v_idx)\n",
    "            v = nonparents[v_idx]\n",
    "            add_edge!(G, v, u)\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    \n",
    "    return G\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Involution function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function digraph_involution(cur_tr, fwd_choices, fwd_ret, prop_args)\n",
    "    \n",
    "    # Update the trace \n",
    "    new_G = fwd_ret\n",
    "    old_G = cur_tr[:G]\n",
    "    update_choices = Gen.choicemap()\n",
    "    update_choices[:G] = new_G\n",
    "    new_tr, weight, retdiff, discard = Gen.update(cur_tr, Gen.get_args(cur_tr), (), update_choices)\n",
    "    \n",
    "    # figure out what has changed\n",
    "    fwd_u_idx = fwd_choices[:u_idx]\n",
    "    sorted_vertices = sort(collect(old_G.vertices))\n",
    "    fwd_u = sorted_vertices[fwd_u_idx]\n",
    "    fwd_v_idx = fwd_choices[:v_idx]\n",
    "    \n",
    "    # Deduce the correct backward choices\n",
    "    bwd_choices = Gen.choicemap()\n",
    "    if fwd_choices[:remove_edge] # an edge was removed -- we must add it back.\n",
    "        \n",
    "        fwd_parents = collect(in_neighbors(old_G, fwd_u))\n",
    "        fwd_v = sort(fwd_parents)[fwd_v_idx]\n",
    "        bwd_nonparents = sort(collect(setdiff(new_G.vertices,in_neighbors(new_G, fwd_u))))\n",
    "        bwd_v_idx = indexin([fwd_v], bwd_nonparents)[1]\n",
    "        \n",
    "        bwd_choices[:u_idx] = fwd_u_idx \n",
    "        bwd_choices[:remove_edge] = false\n",
    "        bwd_choices[:reverse_edge] = false\n",
    "        bwd_choices[:v_idx] = bwd_v_idx\n",
    "        \n",
    "    else\n",
    "        if fwd_choices[:reverse_edge] # an edge was reversed -- reverse it back.\n",
    "            \n",
    "            fwd_exc_outneighbors = sort(collect(setdiff(out_neighbors(old_G, fwd_u), in_neighbors(old_G, fwd_u))))\n",
    "            fwd_v = fwd_exc_outneighbors[fwd_v_idx]\n",
    "            bwd_u_idx = indexin([fwd_v], sorted_vertices)[1]\n",
    "            bwd_exc_outneighbors = sort(collect(setdiff(out_neighbors(new_G, fwd_v), in_neighbors(new_G, fwd_v))))\n",
    "            bwd_v_idx = indexin([fwd_u], bwd_exc_outneighbors)[1]\n",
    "            \n",
    "            bwd_choices[:u_idx] = bwd_u_idx\n",
    "            bwd_choices[:remove_edge] = false\n",
    "            bwd_choices[:reverse_edge] = true\n",
    "            bwd_choices[:v_idx] = bwd_v_idx\n",
    "            \n",
    "        else # an edge was added -- remove it.\n",
    "            \n",
    "            fwd_nonparents = sort(collect(setdiff(old_G.vertices, in_neighbors(old_G, fwd_u))))\n",
    "            fwd_v = fwd_nonparents[fwd_v_idx]\n",
    "            bwd_parents = sort(collect(in_neighbors(new_G, fwd_u)))\n",
    "            bwd_v_idx = indexin([fwd_v], bwd_parents)[1]\n",
    "            \n",
    "            bwd_choices[:u_idx] = fwd_u_idx\n",
    "            bwd_choices[:remove_edge] = true\n",
    "            bwd_choices[:v_idx] = bwd_v_idx\n",
    "            \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return new_tr, bwd_choices, weight\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let's visualize our Metropolis-Hastings sampler. This is often the best way to check whether a sampler is working as expected.\n",
    "\n",
    "There's a one-to-one mapping between (i) directed graphs and (ii) bipartite undirected graphs. We'll visualize our directed graphs as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function visualize_digraph(dg::DiGraph)\n",
    "    v = sort(collect(dg.vertices))\n",
    "    ys = length(v):-1:1\n",
    "    y_dict = Dict([vert => ys[i] for (i,vert) in enumerate(v)])\n",
    "    xs = [1.0, length(v)]\n",
    "    \n",
    "    p = Plots.plot(xlim=(0.0, length(v)+1), ylim=(0.0, length(v)+1), \n",
    "    legend=false, framestyle=:none, aspect_ratio=1.0)\n",
    "    Plots.yticks!(p, ys, v)\n",
    "    \n",
    "    for i=1:size(dg.edges)[1]\n",
    "        e = dg.edges[i,:]\n",
    "        Plots.plot!(p, xs, [y_dict[e[1]]; y_dict[e[2]]], c=:black)\n",
    "    end\n",
    "    for j=1:length(v)\n",
    "        Plots.scatter!(p, xs, [y_dict[v[j]]; y_dict[v[j]]], markersize=60.0/length(v), markercolor=:gray)\n",
    "        Plots.annotate!(p, xs[1]-0.2, y_dict[v[j]], Plots.text(v[j], :right, 10))\n",
    "        Plots.annotate!(p, xs[2]+0.2, y_dict[v[j]], Plots.text(v[j], :left, 10))\n",
    "    end\n",
    "    \n",
    "    return p\n",
    "end\n",
    "        \n",
    "function visualize_digraph_weighted(dg::DiGraph, weights::Dict)\n",
    "    v = sort(collect(dg.vertices))\n",
    "    ys = length(v):-1:1\n",
    "    y_dict = Dict([vert => ys[i] for (i,vert) in enumerate(v)])\n",
    "    xs = [1.0, length(v)]\n",
    "    \n",
    "    p = Plots.plot(xlim=(0.0, length(v)+1), ylim=(0.0, length(v)+1), \n",
    "    legend=false, framestyle=:none, aspect_ratio=1.0)\n",
    "    Plots.yticks!(p, ys, v)\n",
    "    \n",
    "    for (e, w) in weights\n",
    "        Plots.plot!(p, xs, [y_dict[e[1]]; y_dict[e[2]]], \n",
    "                    color=Plots.RGBA(0,0,0, w), lw=3.0)\n",
    "    end\n",
    "    for j=1:length(v)\n",
    "        Plots.scatter!(p, xs, [y_dict[v[j]]; y_dict[v[j]]], markersize=60.0/length(v), markercolor=:gray)\n",
    "        Plots.annotate!(p, xs[1]-0.2, y_dict[v[j]], Plots.text(v[j], :right, 10))\n",
    "        Plots.annotate!(p, xs[2]+0.2, y_dict[v[j]], Plots.text(v[j], :left, 10))\n",
    "    end\n",
    "    \n",
    "    return p\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = visualize_digraph(reference_dg)\n",
    "Plots.title!(p, \"Reference Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function animate_dbn_sampling(dg::DiGraph, model, model_args, proposal, proposal_args, \n",
    "                              involution, n_samples, thinning, title, gif_file_name)\n",
    "   \n",
    "    tr, _ = Gen.generate(model, model_args)\n",
    "    anim = @animate for i=1:n_samples\n",
    "        for t=1:thinning\n",
    "            tr, _ = Gen.metropolis_hastings(tr, proposal, proposal_args, involution)\n",
    "        end\n",
    "        p = visualize_digraph(tr[:G])\n",
    "        curtitle = replace(title, \"__SAMPLES__\"=>i)\n",
    "        Plots.title!(p, curtitle)\n",
    "        if i%thinning == 0\n",
    "            println(i, \" steps completed\")\n",
    "        end\n",
    "    end \n",
    "    gif(anim, gif_file_name)\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "function animate_dbn_sampling_density(dg::DiGraph, model, model_args, proposal, proposal_args, involution, \n",
    "                                      n_samples, thinning, median_weight, title, gif_file_name)\n",
    "    \n",
    "    tr, _ = Gen.generate(model, model_args)\n",
    "    count_dict = Dict()\n",
    "    \n",
    "    anim = @animate for i=1:n_samples\n",
    "        \n",
    "        for t=1:thinning\n",
    "            tr, _ = Gen.metropolis_hastings(tr, proposal, proposal_args, involution)\n",
    "        end\n",
    "        \n",
    "        G = tr[:G]\n",
    "        for j=1:size(G.edges)[1]\n",
    "            e = G.edges[j,:]\n",
    "            count_dict[e] = get(count_dict, e, 0) + 1\n",
    "        end\n",
    "        \n",
    "        weight_dict = Dict([e => (1.0*count/i)^(-1.0/log2(median_weight)) for (e, count) in count_dict])\n",
    "        #println(weight_dict)\n",
    "        p = visualize_digraph_weighted(G, weight_dict)\n",
    "        curtitle = replace(title, \"__SAMPLES__\"=>i)\n",
    "        Plots.title!(p, curtitle)\n",
    "    end\n",
    "       \n",
    "    fig = gif(anim, gif_file_name)\n",
    "    \n",
    "    return count_dict\n",
    "end\n",
    "   \n",
    "\n",
    "function graph_diff(new_g::DiGraph{T}, old_g::DiGraph{T}) where T\n",
    "    new_e = Set([new_g.edges[i,:] for i=1:size(new_g.edges)[1]])\n",
    "    old_e = Set([old_g.edges[i,:] for i=1:size(old_g.edges)[1]])\n",
    "    \n",
    "    new_exc = setdiff(new_e, old_e)\n",
    "    old_exc = setdiff(old_e, new_e)\n",
    "    if length(new_exc) > 0 && length(old_exc) == 0\n",
    "        return \"add\", first(new_exc)\n",
    "    elseif length(old_exc) > 0 && length(new_exc) == 0\n",
    "        return \"remove\", first(old_exc)\n",
    "    elseif length(old_exc) > 0 && length(new_exc) > 0\n",
    "        return \"reverse\", (first(old_exc), first(new_exc))\n",
    "    else\n",
    "        return \"none\", nothing\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function animate_dbn_exploration(dg::DiGraph, model, model_args, proposal, proposal_args, \n",
    "                                 involution, n_iterations, title, gif_file_name)\n",
    "    \n",
    "    v = sort(collect(dg.vertices))\n",
    "    ys = length(v):-1:1\n",
    "    y_dict = Dict([vert => ys[i] for (i,vert) in enumerate(v)])\n",
    "    xs = [1.0, length(v)]\n",
    "    \n",
    "    tr, _ = Gen.generate(model, model_args)\n",
    "    rejections = 0\n",
    "    anim = @animate for i=1:n_iterations\n",
    "        old_G = copy(tr[:G])\n",
    "        tr, acc = Gen.metropolis_hastings(tr, proposal, proposal_args, involution)\n",
    "        if !acc\n",
    "            rejections += 1\n",
    "        end\n",
    "        new_G = tr[:G]\n",
    "        p = visualize_digraph(new_G)\n",
    "        curtitle = replace(title, \"__STEPS__\"=>i)\n",
    "        curtitle = replace(curtitle, \"__REJECTIONS__\"=>rejections)\n",
    "        Plots.title!(p, curtitle)\n",
    "        diffstr, e = graph_diff(new_G, old_G)\n",
    "        if diffstr == \"add\"\n",
    "            plot!(p, xs, [y_dict[e[1]]; y_dict[e[2]]], color=:blue, lw=4.0)\n",
    "        elseif diffstr == \"remove\"\n",
    "            plot!(p, xs, [y_dict[e[1]]; y_dict[e[2]]], color=:red, lw=4.0)\n",
    "        elseif diffstr == \"reverse\"\n",
    "            plot!(p, xs, [y_dict[e[1][1]]; y_dict[e[1][2]]], color=:yellow, lw=4.0)\n",
    "            plot!(p, xs, [y_dict[e[2][1]]; y_dict[e[2][2]]], color=:yellow, lw=4.0)\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    fig = gif(anim, gif_file_name)\n",
    "    \n",
    "    return 1.0*rejections/n_iterations\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function prior_model(lambda::Float64)\n",
    "    @trace(graphprior(lambda, reference_dg), :G)\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lambda = 3.0\n",
    "n_steps = 1000\n",
    "#med_indegs = [1.0; 2.0; 3.0; 3.7; 4.0; 5.0; 6.0]\n",
    "med_indegs = [0.25; 0.5; 0.75]\n",
    "rejection_rates = []\n",
    "for med_indeg in med_indegs\n",
    "    rr = animate_dbn_exploration(reference_dg, prior_model, (lambda,), digraph_proposal, (med_indeg,), \n",
    "                                 digraph_involution, n_steps,\n",
    "                                 \"Graph Exploration: __STEPS__ steps (__REJECTIONS__ rejected)\",\n",
    "                                 \"dg-explore-indeg$(med_indeg)-steps$(n_steps)-lambda$(lambda).gif\");\n",
    "    push!(rejection_rates, rr)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejection_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thinnings = [100; 500; 1000]\n",
    "n_samples = 100\n",
    "for thinning in thinnings\n",
    "    animate_dbn_sampling(reference_dg, prior_model, (lambda,), digraph_proposal, (2.0,), \n",
    "                         digraph_involution, n_samples, thinning,\n",
    "                         \"Graph Prior Sampling: __SAMPLES__ samples\",\n",
    "                         \"dg-sample-samples$(n_samples)-thinning$(thinning)-lambda$(lambda).gif\"\n",
    "                        );\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thinning in thinnings\n",
    "    animate_dbn_sampling_density(reference_dg, prior_model, (3.0,), digraph_proposal, (2.0,), \n",
    "                                 digraph_involution, n_samples, thinning, 0.50,\n",
    "                                 \"Graph Prior Edge Marginals: __SAMPLES__ samples\",\n",
    "                                 \"dg-density-samples$(n_samples)-thinning$(thinning)-lambda$(lambda).gif\");\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function simple_sampling(dg, model, model_args, proposal, proposal_args,\n",
    "                         involution, n_samples, thinning)\n",
    "    \n",
    "    tr, _ = Gen.generate(model, model_args)\n",
    "    for i=1:n_samples\n",
    "        for t=1:thinning\n",
    "            tr, _ = Gen.metropolis_hastings(tr, proposal, proposal_args, involution)\n",
    "        end\n",
    "    end        \n",
    "end    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time begin\n",
    "    simple_sampling(reference_dg, dumb_model, (3.0,), digraph_proposal, (2.0,), digraph_involution,\n",
    "                       1000, 100)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
